# NICE 56期 | RL is All Tool Learning Need

## 主题

RL is All Tool Learning Need

## 时间

2025.4.27 10:30

## 内容

- ToolRL: Reward is All Tool Learning Needs
- OTC: Optimal Tool Calls via Reinforcement Learning
两篇文章共同解决了：如何让大语言模型通过强化学习又好又准地使用工具？
ToolRL 解决的是通过强化学习教会模型如何使用工具（如何“好”）：我们提出了一整套基于强化学习的奖励设计，帮助模型学会在多步任务中正确、灵活地选择并调用工具。
OTC-PO 则进一步解决教会模型如何少用、精用工具（如何“准”）：我们引入效率权重奖励，让模型在保证正确率的前提下，主动权衡工具调用的收益与代价，实现工具使用的高效性与成本控制。
二者构成一前一后的闭环：前者建立能力寻找最优奖励，后者约束行为探索最优效率，合力推动构建更强、更省、更“懂事”的 AI 工具使用智能体。

ToolRL: Reward is All Tool Learning Needs 
链接: https://arxiv.org/abs/2504.13958

当前大语言模型在使用工具时大多依赖人工标注的监督微调，但这种方式难以应对复杂的工具使用场景。本文提出一种全新的训练范式——ToolRL，首次系统性地探索如何为“工具选择与使用”任务设计强化学习中的奖励函数。研究团队基于这一奖励设计，采用了改进版的策略优化算法 GRPO，对模型进行训练，效果显著提升。这项工作不仅提升了模型的泛化能力，还让语言模型展现出更强的主动性和元认知能力，为未来更智能的人工智能体训练奠定了重要基础。 

OTC: Optimal Tool Calls via Reinforcement Learning 
链接: https://arxiv.org/pdf/2504.14870

不同模型面对不同问题的时候，存在最优的工具调用次数，即最少的工具调用来回答问题。而当前RL仅仅针对最终答案进行优化，忽略了大模型在工具交互过程中的行为。我们首次提出基于最优工具调用的强化学习算法，使得模型最大程度保持正确率的同时，极大地提高工具的使用效率。具体来说，我们是第一个关注大模型在强化学习中工具使用行为尤其是工具推理效率的工作；第一个提出工具生产力的概念，使得模型不仅仅关注收益，并且关注工具调用成本；第一个观测到大模型的认知卸载现象，即模型越大，越倾向于使用外部工具，从而损坏或者浪费自身的推理能力。我们的方法简单，高效，通用，泛化能力强。最小化外部工具调用就是最大化激发模型的内在推理能力，从而让模型学会仅仅在必要的时候使用外部工具并且由于更强的推理能力从而更好的使用外部工具。简单来说，OTC-PO = 少查工具 + 多用脑子 + 多用脑子查工具。

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
