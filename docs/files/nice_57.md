# NICE 57期 | LUFFY：让推理模型实现“即学即用”的强化学习训练方法

## 主题

LUFFY：让推理模型实现“即学即用”的强化学习训练方法

## 时间

2025.4.29 20:00

## 内容

paper: Learning to Reason under Off-Policy Guidance
link: https://arxiv.org/abs/2504.14945

大型推理模型(LRMs)的最新进展表明，通过使用简单的基于规则的奖励进行强化学习(RL)，可以形成诸如多步推理和自我反思等复杂行为。然而，现有的零强化学习(zero-RL)方法本质上是"同策略"(on-policy)的，这限制了模型只能从自身输出中学习，无法获得超出其初始能力的推理能力。我们提出了LUFFY(在离策略指导下学习推理，Learning to reason Under oFF-policY guidance)，这是一个通过离策略推理轨迹来增强零强化学习的框架。LUFFY在训练过程中通过结合离策略示范和同策略推演，动态平衡模仿和探索。值得注意的是，我们提出了通过正则化重要性采样进行策略塑造，以避免混合策略训练中的表面和僵化模仿。LUFFY在六个数学基准测试中平均提高了7.0分，在分布外任务中提升6.2 分。它还大幅超越了基于模仿的监督微调(SFT)，特别是在泛化能力方面。分析表明，LUFFY不仅能有效模仿，还能探索超越示范的空间，为使用离策略指导训练可泛化的推理模型提供了一条可扩展的路径。 

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
