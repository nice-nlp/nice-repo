# NICE 23期 | 你的模型真的擅长数学吗？MathCheck：大模型数学推理能力的新评估范式

## 主题

你的模型真的擅长数学吗？MathCheck：大模型数学推理能力的新评估范式

## 时间

2024.8.24 10:30-11:30 周六

## 内容

内容大纲
1.背景-大模型数学推理能力研究现状
2.我们为什么需要更好的评估范式？
3.MathCheck评估框架设计
4.数据构建方法与数据集介绍
5.大模型在MathCheck上的性能表现
6. 数学推理中的行为分析与发现
7. 总结与展望
8. QA

引言
数学推理能力是大语言模型智能水平的重要反映。如何评估大模型真实的数学推理能力，甚至反映现实场景中的用户体验，已成为一个关键问题。目前的评估范式主要集中于求解数学问题上的性能表现，这带来了模型过拟合的巨大风险，无法准确反映模型真正的数学推理能力。
在本文中我们提出，如果模型真的理解了一个数学问题，那么它应该能够鲁棒地将其应用于关于这道题的各种数学任务。受此启发，我们提出了MathCheck，一个精心设计的测试矩阵，用于评测大模型在数学推理上的任务泛化性和推理鲁棒性。MathChck包含了多个数学推理任务和鲁棒性问题变种，以对数学推理能力进行综合评估和推理行为分析。利用MathCheck，我们构建MathCheck-GSM (文本应用题) 和 MathCheck-GEO (多模态几何题) 来分别评估文本推理能力和多模态推理能力。
我们在MathCheck-GSM和 MathCheck-GEO上评估了21个大语言模型和11个多模态大语言模型。评测结果表明，前沿模型如GPT-4o等在MathCheck上继续表现出色，但是其他模型表现出显著的下降。进一步的实验表明，与传统评估范式相比，MathCheck更能反映出真实的数学推理能力。此外，借助MathCheck我们可以直观地对模型的推理行为进行行为分析，并得到一些有趣的发现。希望我们的研究和观察能够推动更全面地评估大模型推理能力研究。我们的项目地址: https://mathcheck.github.io/

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
