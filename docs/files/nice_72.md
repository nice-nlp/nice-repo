# NICE 72期 | NICE×UIUC：伊利诺伊大学香槟分校专场分享会

## 主题

NICE×UIUC：伊利诺伊大学香槟分校专场分享会

## 时间

2025.06.29（周日）10:00

## 内容

论文：Taming Knowledge Conflicts in Language Models

在本文中，我们针对大语言模型在参数记忆（parametric memory）与上下文知识（contextual information）发生矛盾时的 “知识冲突”（knowledge conflict）现象展开系统性研究。我们首先发现，先前被视为分别承载记忆或上下文的注意力头实际上常常同时编码这两类信息（我们称之为 the superposition of parametric memory and contextual information）。基于这一洞察，我们提出了无需微调、仅依赖 双轮推理 即可灵活控制知识来源的轻量级方法 JuICE：第一轮自动定位能稳定引导模型行为的关键注意力头；第二轮对其输出进行正负缩放，从而在推理阶段有针对性地放大参数记忆或外部上下文的影响。实验证明，JuICE 在 6 个主流开源模型和 11 个数据集上，于“强化记忆”和“增强上下文依赖”两类任务均取得一致的 SOTA 提升；理论分析进一步证明了“记忆-上下文叠加”现象的普遍性以及 JuICE 的有效性。我们的工作不仅澄清了知识冲突的本质，也为 RAG、工具调用等场景中的可靠知识控制提供了简单而实用的解决方案。 

论文：Theory of Agent: From Definition to Behavior and Objective

大语言模型（LLMs）已从单纯的文本生成工具，迅速演变为能够在有限人类监督下自主规划和执行复杂任务的智能体（如OpenAI深度研究、Manus和Alita等）。然而，随着这些系统日益具备自主性和代理能力，一些根本性问题仍未解决：什么是智能体？其最优行为是什么？如何在实践中实现这种最优性？在本次分享中，我们试图建立一套系统的智能体理论，将其定义为基于工具使用的决策者。具体而言，我们首先提出一种工具化视角的统一框架，归纳智能体的所有行为归纳为工具调用，并阐述关于知识边界（智能体所知）与决策边界（智能体所行）的三项核心原则。我们提倡真正的自主性取决于决策边界与知识边界的对齐，目标是通过最小化现实世界中的外部动作，高效实现预设目标。最后，我们提供一套可操作、可扩展的路线图，以实现真正自主的智能体，并展望未来在安全性、个性化和通用性等方面的关键挑战与发展方向。

论文：ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges

本文提出了一个面向现实世界数学建模问题的新基准测试集 ModelingBench，涵盖多个跨学科领域的问题，旨在挑战和评估大语言模型在现实挑战中的推理能力。同时，本文推出了一个多智能体系统 ModelingAgent，模拟人类团队协作，通过角色分工和自我进化机制，能够有效解决复杂建模任务。此外，本文构建了一个自动评估框架 ModelingJudge，使模型输出可被多角色专家进行仿真评价。实验展现了当下语言模型答案已经能够通过图灵测试，但在专家评价下仍在建模创新性，报告完整性等方面有待提升。

论文：RM-R1: Reward Modeling as Reasoning

在本文中，我们提出了一类全新的生成式奖励模型——推理奖励模型（Reasoning Reward Models, ReasRMs），将奖励建模重新表述为一个推理任务。我们设计了一个面向推理的训练流程，并据此训练了一组模型 RM-R1。RM-R1 的核心机制是 Chain-of-Rubrics （CoR）：模型能够为每个样本自主生成聊天评价标准或数学/编程题的解答，并据此对候选答案进行评估。RM-R1 的训练包括两个关键阶段：（1）高质量推理链的蒸馏，（2）结合可验证奖励的强化学习。在多个奖励模型评测基准上，我们的模型在平均表现上达到了当前最优，最高相比更大规模的开源模型（如 INF-ORM-Llama3.1-70B）和专有模型（如 GPT-4o）提升达 4.9%。 

论文：An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents

强化学习（Reinforcement Learning, RL）在训练具备复杂推理能力的大型语言模型（Large Language Models, LLMs）方面展现出巨大潜力，推动其在现实世界问题求解中的应用。近年来，RL 也被广泛用于构建融合搜索引擎与语言模型推理能力的智能搜索代理，展现出强大的综合检索与推理能力。然而，如何最优设计此类搜索代理仍缺乏系统性理解。特别是关于以下几个关键因素仍有待深入探索：(1) 奖励函数的设计，(2) 底层语言模型的选择与特性，(3) 搜索引擎在强化学习过程中的作用。在本研究中，我们围绕上述问题展开全面实证分析，提出一系列可操作的设计建议。主要发现包括：（1）格式化奖励（format rewards）对最终性能提升有效，而中间检索奖励的影响有限；（2）LLM 的规模与初始化（通用型 vs. 专注推理型）显著影响训练效果；（3）搜索引擎的选择在训练动态与模型推理稳健性方面起到关键作用。本工作为构建高效、稳健的 LLM 搜索代理提供了重要的实证依据和设计指导。 

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
