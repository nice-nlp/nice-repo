# NICE 77期 | 大模型如何学习？揭秘语言模型"记忆"与"泛化"的平衡艺术

## 主题

大模型如何学习？揭秘语言模型"记忆"与"泛化"的平衡艺术

## 时间

2025.08.09 周六 10:30

## 内容

论文：Generalization vs. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data
论文地址：https://openreview.net/forum?id=0LaybrPql4&referrer=%5Bthe%20profile%20of%20Alon%20Albalak%5D(%2Fprofile%3Fid%3D~Alon_Albalak1)
尽管大型语言模型（LLMs）在现实世界应用中已被证明具有显著效用，我们对它们如何利用大规模预训练文本语料来实现这些能力仍缺乏深入理解。在本研究中，我们通过对其训练数据进行全面的 n-gram 分析，探讨了大规模预训练 LLM 中泛化与记忆之间的相互作用。我们的实验聚焦于三类通用任务类型：翻译、问答和多项选择推理。基于不同规模的开源 LLM 及其预训练语料，我们观察到，随着模型规模的扩大，与任务相关的 n-gram 对数据变得愈发重要，从而带来了任务表现的提升、记忆化程度的降低、更强的泛化能力，以及新兴能力的出现。我们的结果支持这样的假设：LLM 的能力来源于对记忆与泛化之间微妙平衡的把握，而这一平衡依赖于充足的任务相关预训练数据。我们的研究也为未来更大规模的分析指明了方向，有望进一步加深我们对这些模型的理解。

## 嘉宾

王心怡，普林斯顿大学语言与智能实验室的博士后研究员。从加州大学圣塔芭芭拉分校（UCSB）获得博士学位，导师是William Yang Wang。曾获得 J.P. Morgan 人工智能博士奖学金以及 UCSB 计算机科学杰出论文奖。研究致力于从原理上理解大型基础模型。
个人主页：https://wangxinyilinda.github.io/

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
