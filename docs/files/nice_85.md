# NICE 85期 | ARPO: 让你的Agent在关键时刻“多探索一步”！

## 主题

ARPO: 让你的Agent在关键时刻“多探索一步”！

## 时间

2025.9.3 19:00

## 内容

论文信息
标题
Agentic Reinforced Policy Optimization
地址
https://arxiv.org/abs/2507.19849
GitHub
https://github.com/RUC-NLPIR/ARPO
X (Twitter)
https://x.com/_akhaliq/status/1950172418250547478
内容介绍
近年来，可验证奖励的大规模强化学习在单轮推理任务中充分释放了大语言模型的潜力。然而，在开放式推理场景下，LLM不仅需要具备长程规划与自适应决策能力，还需与外部工具进行动态的多轮交互。这催生了智能体强化学习（Agentic RL）这一新范式，将训练从静态求解转向动态的智能体-环境交互式推理。现有智能体强化学习方法多直接采用样本级算法（如 GRPO、DAPO等），通过独立采样完整的工具调用轨迹进行强化训练。这种方式常面临奖励稀疏、工具过度调用等问题。更重要的是，这类方法忽略了智能体多轮交互的特性，低估了工具调用过程中每一步的细粒度行为探索。图1：左图展示大模型在调用工具后的高熵现象，右图对比ARPO与基线性能。
通过对大模型在深度搜索任务中的token熵分布进行分析，研究发现模型在每次工具调用后的初始生成阶段熵值显著升高，说明外部工具反馈会引入高不确定性，而这正是现有方法未充分利用的探索契机。针对上述发现，我们提出智能体强化策略优化（Agentic Reinforced Policy Optimization, ARPO），其核心思想是在高熵工具调用步骤中，自适应地分支采样，在 Rollout 阶段探索多样化的推理路径。该文章同时荣登 Huggingface Paper 日榜，周榜第一名🏆！图2：ARPO的基于熵驱动的自适应rollout机制，结合全局探索与局部高熵节点分支。图3：ARPO核心组件的示意图：左侧：基于熵的自适应束搜索原理；右侧：优势归因估计。
具体来说，我们的贡献如下：
我们量化了大模型在工具结合推理过程中的token熵变化，揭示了样本级强化学算法在对齐智能体方面的固有限制。
我们提出了ARPO算法，引入基于熵的自适应Rollout机制，在保持全局采样的同时，在高熵工具调用步骤中鼓励分支采样。此外，ARPO 结合优势归因估计，帮助大模型更好地内化步骤级工具使用行为中的优势差异。
除了启发式动机，我们还从理论层面上论证了在大模型智能体训练中引入 ARPO 算法的合理性。
在 13 个挑战性的评测集上的实验表明，ARPO在仅使用一半工具调用训练预算的情况下，性能稳定优于主流强化学习算法，为探索Agentic RL提供了具备可扩展性的新方案。

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
