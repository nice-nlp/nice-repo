# NICE 32期 | 迈向可信的AI：探索安全、负责的大语言模型

## 主题

迈向可信的AI：探索安全、负责的大语言模型

## 时间

2024.11.7 20:00-21:00 周四

## 内容

论文1：MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability （NeurIPS 2024)
链接：https://arxiv.org/pdf/2405.14488
单位：哈工大、度小满
论文2：Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning
链接：https://arxiv.org/pdf/2410.04524
单位：哈工大、港中文

引言
随着大语言模型的广泛应用，对抗攻击与防御成为了研究领域中的一个重要课题。对抗攻击指的是通过精心设计的输入，诱导模型生成错误或有害的输出，进而影响模型的正常行为。尽管一些模型经过RLHF与人类价值观进行对齐，但模型（如Llama）仍然很容易遭受到越狱攻击。为了应对这些威胁，目前的研究已经提出一些防御策略，如解码时令牌概率的重构、输入的预处理和输出内容的安全监测等。然而，我们注意到尽管现有的防御策略改善模型的安全性，但往往忽略了模型的可用性。简单来说，现有的防御策略通常导致模型的安全性和可用性之间存在一个“跷跷板”现象。为了解决这一挑战，我们提出了Mixing of Glad and Unwiiling Responders（MoGU）框架，该框架通过引入动态路由机制来平衡模型的两种情绪变体（积极和拒绝）来实现保证模型可用性的同时改善其安全性的目的。
此外，ICLR24的一篇工作表明指令微调通常会损害语言模型的安全性，尽管在指令微调期间使用的全部是良性指令。为了探究这一问题，我们分析了模型内部模块（包括Q/K/V/O/Gate/Down/Up）对于模型安全的鲁棒性。在我们的分析中，我们观测到模块的鲁棒性随着模块的类型和层数有规律的变化。简单来说，1）模型层数越深（越靠近输出层），模块越鲁棒。2）Q/K模块相对于其他模块更加敏感。3）模型的安全受到模块之间的协同作用，而不是仅仅受特定的模块影响。基于我们的发现，我们设计了Modular Layer-wise Learning Rate（ML-LR）策略。该策略首先通过搜素算法识别出一组鲁棒的模块集合，随后为该鲁棒的模块集合分配一个标准的学习率，而为其他模块分配一个较小的学习率。实验结果表明，该策略在几乎不影响指令微调作用的前提下，极大地缓解了指令微调带来的安全风险。

分享内容大纲
- 背景：
  1. 大语言模型
  2. 对抗攻击
  3. 防御护栏
- MoGU安全性增强框架：
  1. 动机
  2. 方法
  3. 实验设置
  4. 实验结果与分析
- 迈向安全的指令微调：
  1. 动机
  2. 方法
  3. 实验设置
  4. 实验结果与分析
- 总结

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
