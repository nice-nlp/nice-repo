# NICE 69期 | 安全/可信论文分享@ICML&ACL2025

## 主题

安全/可信论文分享@ICML&ACL2025

## 时间

2025.06.18 (周三) 09:00

## 内容

论文：LongSafety: Evaluating Long-Context Safety of Large Language Models [ACL 2025]

大型语言模型（LLM）在理解和生成长文本方面不断取得进展，而长上下文也为模型带来了新的安全隐患。然而，LLM 在长文本任务中的安全性仍未得到充分探索，这在其安全性评估和改进方面都留下了显著空白。为解决这一问题，我们提出了 LongSafety，首个专用于 LLM 长文本生成任务安全性评估的基准。LongSafety 涵盖 7 类安全问题和 6 类面向用户的长上下文任务，共包含 1543 个测例，每段上下文平均包含 5424 个单词。我们对 16 个 LLM 的评估揭示了模型在长文本安全任务上存在显著的安全漏洞，大多数模型的安全率低于 55%，且在特定安全问题和任务类型上存在明显缺陷。我们的研究结果还表明，模型在短文本场景中表现出的强安全性并不一定与长文本任务的安全性相关，这凸显了改善长文本安全能力所面临的独特挑战。我们还发现，相关上下文和更长的文本输入可能会加剧长文本场景中的安全风险，表明对长文本安全挑战的关注至关重要。 

论文：UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models [ACL 2025]

当前大模型无法总是正确表达事实知识——尤其对于那些自身知识边界比较模糊的问题，即使学习过相关的知识却也很难答对问题，而对没学过的问题却会给出错误答案，存在“感知”和“表达”的沟壑，造成严重的幻觉问题。为了提高大模型的事实性表达，我们首先使用不确定性估计来准确度量和表征模型的知识边界，然后再将这些融合了知识边界的不确定性信息用于大模型的对齐训练，如同人类在回答问题前先进行仔细的知识回顾和估计，从而最终使大模型对那些原本知道但不确定的问题自信的回答对，而对那些没学过的知识进行拒答，提高了大模型的事实性表达，有效缓解了模型在知识类任务的幻觉问题。 

[ICML 2025] Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning

论文：Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints [ACL 2025]

基于梯度的白盒越狱攻击算法是一类有效的越狱攻击算法，但是其迁移性始终缺乏深入探究。迁移性是梯度越狱攻击能够攻击GPT-4这类闭源模型的重要保证，也能大幅减少攻击大规模模型的攻击成本。我们针对梯度越狱对抗攻击提出了一个概念性的迁移性解释框架，并且发现原先强迫式的优化目标存在根本性的迁移缺陷：superfluous constraints（错误的冗余约束）。这些superfluous constraints主要包含response pattern constraint（隐性指定不适配的回复模式）和token tail constraint（冗余的尾部词元约束），均源于优化目标的强迫性模式。通过移除这些superfluous constraints，我们提出一种修正的可迁移优化算法，采用了更加灵活和集中于越狱的优化目标，将Llama3上的平均迁移攻击成功率（迁移至Llama2 & GPT-4 & Gemma-It & Qwen2 & ...）从18.4%提高到50.3%。我们还发现，去除superfluous constraints不仅提高了攻击成功率，也可以将迁移越狱行为变得更加可控和稳定。 

论文：AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection [ACL 2025]

大语言模型 (LLM) 的快速发展使其能够部署为自主代理，用于在动态环境中处理复杂任务。这些 LLM 展现出强大的问题解决能力和对多方面场景的适应性。安全风险。现有的防御机制未能自适应有效地降低这些风险。本文提出了 AGrail，一种终身代理护栏，旨在增强 LLM 代理的安全性，它具有自适应安全检查生成、有效的安全检查优化以及工具兼容性和灵活性。大量实验表明，AGrail 不仅在应对特定任务和系统风险方面表现出色，而且还展现出在不同 LLM 代理任务之间的可迁移性。 

论文：Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models [ICML 2025]

多模态大语言模型在视觉问答微调过程中，即便将与任务完全无关的随机“水印”隐私嵌入训练图像，也会因小批量训练产生的偶发相关而被模型参数意外记忆。我们从理论上证明小批量梯度更新会因采样波动使隐私向量与标签出现非零样本协方差，进而在梯度中留下隐私相关分量并被模型参数吸收 。实证方面，在多套 VQA 数据集上调控嵌入比例后，我们发现嵌入的隐私对模型下游任务几乎没有影响，但梯度方向偏移幅度与常见图形数据增强相当，证明模型确实为编码这些水印付出了更新成本。我们进一步设计受控探针方法构造分布一致但互不重叠的两组隐私水印，并在各层隐藏表示上训练线性探针区分见过和未见过的隐私，证明模型自中层起便能无意识地编码这些任务无关的隐私。 

论文：Safety Reasoning with Guidelines [ICML 2025]

我们探究了为何拒绝回复训练 Refusal Training在面对分布攻击时的泛化能力较差，通过BoN采样，我们发现能够持续且显著提高防御成功率，这说明经过拒绝回复安全对齐的模型本身具有相关的知识能够防御这些分布外攻击，只是拒绝回复这一训练形式使得其限入了简单的模式学习，限制了相关知识的使用。基于此，我们提出了一套利用指南进行安全推理的方法，显著提升了模型对自身知识的运用，显著降低了分布外攻击的攻击成功率。我们的方法SRG在不同数据规模（4k to 30k)，不同模型规模（7b to 70b)，不同模型类型（llama, gemma, mistral, qwen)，不同教师模型（GPT4o, Qwen2.5-72b-instruct, DeepSeek R1-32b)下都证明了显著的效果。 

论文：Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets [ICML 2025]

大型语言模型（LLM）作为通用人工智能助理已展现出巨大潜力。为了方便用户在特定场景下应用，许多公司提供了微调服务，允许用户上传数据以定制模型。然而，这一过程带来了新的安全风险：即使用户数据本身是无害的，微调也可能破坏模型的安全对齐机制，导致模型输出不安全的内容。现有的防御方法难以应对多样化的微调数据，往往顾此失彼，无法兼顾模型的安全与下游任务性能。为解决此问题，我们提出了名为Safe Delta的安全感知防御方法。该方法在模型训练后，通过理论分析参数变化带来的安全损失与性能提升，针对性地保留并调整参数变化。在多个不同数据集上实验证明，我们的方法能稳定地维持模型的安全性，同时不影响下游任务能力提升。 

论文：ReLearn: Unlearning via Learning for Large Language Models [ACL 2025]

大模型知识遗忘旨在通过编辑大模型参数实现隐私、偏见等信息的擦除，支撑可靠、可信的大模型应用。当前主流遗忘方法多采用“反向优化”（如梯度上升），旨在抑制特定内容的输出概率。这种仅依赖负向调整的策略，往往使模型难以采样到合理答案，进而破坏输出的连贯性并损害整体语言性能。此外，现有评估指标也过分关注局部遗忘，而忽略了生成内容的流畅度与相关性。鉴于此，本文提出ReLearn：一个基于数据增强与模型精调的高效遗忘框架，以三项新评估指标以全面评估遗忘效果与模型可用性，并从机理层面分析了不同方法遗忘的机制。 

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
