# NICE 66期 | Evaluation论文分享@ICML&ACL2025

## 主题

Evaluation论文分享@ICML&ACL2025

## 时间

2025.6.11 周三 20:00

## 内容

论文：CoSER: Coordinating LLM-Based Persona Simulation of Established Roles
项目地址： https://github.com/Neph0s/CoSER 

论文介绍：角色扮演AI作为大语言模型的重要应用，近年来获得了广泛关注。特别是当扮演小说、动漫中的知名角色时，模型需要获取并有效利用关于这些角色的大量知识。然而，现有的角色扮演 AI 面临两大核心挑战：高质量的真实角色数据集，以及有效的评估方法。 
因此，我们介绍CoSER，一个面向深度AI角色扮演的项目，包含当下最大的真实优质数据集、SoTA开源模型和深入的评估方案，用于高效构建和评估角色扮演 AI。研究论文现已被ICML 2025接收。 

论文：FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging
项目地址：https://bupt-reasoning-lab.github.io/FinanceReasoning 

论文介绍：大推理模型（LRMs）相较大语言模型（LLMs）展现出更强的推理能力，能够处理需要多步推理的复杂任务，例如代码生成、数学求解以及科学问题。然而，更多现实世界的领域特定数值推理任务（例如金融定量分析）需要模型深入理解并应用领域知识，同时基于表格与文本等混合上下文进行复杂数学计算。现有金融数值推理基准在问题标注质量、金融领域知识覆盖度以及推理复杂度方面存在局限，导致难以客观评估LRMs实际推理能力并分析其缺陷。为此，我们提出FinanceReasoning——一个更加可信、全面且具有挑战性的金融数值推理基准，包含2,238道题目，涵盖多样化金融知识。每道题目均包含混合上下文、明确的问题描述、Python格式的解题步骤及精确答案。我们评估了当前领先的6个LRMs和7个LLMs，发现LRMs相较LLMs表现显著提升，但仍存在公式应用错误与数值计算不精确等问题。我们构建并开源了包含3,133个Python函数的金融函数库，通过知识增强和模型组合策略，进一步增强模型在专业领域复杂推理任务上的表现。 

论文：Express 💬 What You See 👀: Can Multimodal LLMs Decode Visual Ciphers with Intuitive Semiosis Comprehension?
项目主页： https://github.com/Eternity-gaga/Emoji2Idiom 

论文介绍：在当前多模态人工智能快速发展的背景下，如何有效弥合视觉与语言之间的语义鸿沟，成为多模态研究领域亟需解决的核心问题。传统视觉问答（VQA）任务中存在明显的模态鸿沟以及对语言先验知识的过度依赖，而人类则凭借直观的符号推理能力（即“符号化”过程），能够灵活地将抽象的视觉符号转化为丰富的语言意义。受此神经认知机制启发，本文聚焦于一种新兴且具有挑战性的任务：大模型是否可以直接理解视觉符号，从视觉层面联想并解码出对应的抽象文本语义。我们关注到，emoji符号天然地桥接视觉与语言模态：emoji符号具有视觉表示，同时又有着约定俗成的特定指示含义，人们常常在日常交流或社交媒体表达中使用emoji替换文字表达。因此，我们选择了在全世界范围内被广泛使用的emoji表情符号作为核心符号，设计从表情符号（emoji）序列图像中生成抽象的语言表达的任务，旨在测试多模态大语言模型（MLLMs）在视觉密码解码中的高层推理能力。 
我们提出了一个全新的多语言、跨文化基准测试——eWe-bench（Express What you SeE），涵盖中文和英文语境下的表情符号与习语配对数据。该数据集构建框架包括三部分：真实世界emoji-text对的检索、文本到emoji的生成以减少偏差，以及结合机器筛选与人工验证的高质量过滤流程，确保数据的视觉敏感性与伦理合规性。此外，我们设计了融合自动评估与人工评估的细粒度评价策略，全面衡量模型的表现。实验结果显示，当前最先进的MLLMs在中文成语任务上的准确率仅为3.3%，远低于人类的67%，暴露出其在处理同音异义关系与多对一映射等复杂语义结构时的重大缺陷。通过对典型案例的分析，我们揭示了现有模型的不足，并为未来改进提供了可行方向。 
本文的主要贡献包括：（1）提出一种新的评估范式，用于衡量MLLMs在视觉直觉符号推理方面的能力；（2）构建了一个高质量、低模态差距的基准数据集eWe-bench；（3）通过实证分析揭示了当前模型在类人多模态理解中的局限性，并为加密分析与高阶认知智能的发展提供启示。本研究为推动具备类人感知能力的多模态智能系统奠定了基础。 

论文：Generative Reward Modeling via Synthetic Criteria Preference Learning 

论文介绍：OpenAI 研究科学家姚顺雨在《The Second Half》中提到了人工智能的研究进入了下半场，关键点变为了：任务定义+好的评估标准。如何获得好的评估标准是一件非常难的事情，因为真实世界的评估是多维度的，细粒度的。此外，获取详细的评估轨迹的代价极高，缺乏扩展性。为了克服这些缺陷，文本提出了生成式奖励建模方法SyncPL，通过将任务评估过程构建为了一棵评估标准的偏好树，通过 inference-time scaling 和预定义的奖励规则来对其进行优化。SyncPL不仅可以以过程监督的方式优化每一个评估标准，还可以为不同的评估标准进行重要性排序。此外，结合o1-like的长思维链优化方法的SyncPL-o1还可以进一步减少训练代价并提升奖励建模的能力，在多个实验基准上展现出了出色的效果。

论文：Mitigating Selection Bias with Node Pruning and Auxiliary Options 

论文介绍：大型语言模型（LLMs）在多项选择题中常出现的选择偏差问题，即模型在回答时对某些选项位置或标签表现出系统性的偏好，无论内容相关性如何。 
为了解决这一问题，作者提出了两种方法： Bias Node Pruning (BNP)：一种参数级的去偏技术，通过识别并剪除模型中导致选择偏差的参数节点，仅需移除约0.002%的模型权重，即可减少偏差并提高多项选择题的准确性。 
Auxiliary Option Injection (AOI)：一种简单有效的提示策略，在原有选项中添加一个“我不知道”的辅助选项，适用于白盒和黑盒模型，有助于减少模型的选择偏差。 
此外，作者还引入了一个新的评估指标——Choice Kullback-Leibler Divergence (CKLD)，用于衡量模型预测分布与真实答案分布之间的差异，从而更准确地评估选择偏差。实验结果表明，这两种方法在多个基准数据集和不同类型的LLMs上均能有效减少选择偏差并提高回答准确性，且与现有技术（如链式思维提示和上下文学习）具有互补性，增强了其实用性。 

论文：Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles
项目地址：https://github.com/wangkevin02/USP 

论文介绍：用户模拟器可以模拟对话过程中人类的交互特性，对于构建高质量数据集以协同训练，模拟用户与大模型多轮交互以动态评估起重要作用。当前主流的基于角色扮演的方法常依赖于预设用户画像，易出现角色混淆，缺乏话语层的真实性与用户模拟的多样性。而直接模拟方法虽然聚焦逼真的话语生成，却忽视了用户的个性特征与对话一致性。 
为了解决这些问题，我们提出了隐式画像驱动的用户模拟器（USP）框架，旨在从人机对话中推理隐性用户画像，以重构更加个性化、逼真的对话。具体而言，我们设计了系统化的画像结构，并构建了一个由大语言模型驱动的画像抽取器；随后结合条件监督微调与基于循环一致性的强化学习，在话语级与对话级联合优化模拟质量；此外，我们还引入多样化画像采样器，以拟合真实用户画像分布并生成多样且合理的画像样本。实验结果表明，USP在对话的真实性与多样性方面显著优于现有强基线，同时在一致性上表现持平。此外，USP在对大模型进行多轮动态评估时，其结果与主流评测基准高度一致，进一步展示了其在真实应用场景中的有效性。 

论文：Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge

论文介绍：LLM-as-a-Judge，基于生成中间的评测推理过程（CoT）从而得到判断，已成为广泛采用的自动评测方法。然而，由于CoT推理往往无法捕捉全面且深入的细节，导致评测结果不够完整，其可靠性因此受到限制。 现有方法通常依赖于多数投票或标准扩展（criteria expansion），但这些方法并不足以有效解决推理中存在的上述局限性。因此，我们提出了一种名为基于群体的比较评测（Crowd-based Comparative Evaluation，简称CCE）的新方法，通过引入额外的群体回复（crowd response）与待评测回复进行对比，从而揭示待评测回复中更深入且更全面的细节。这一过程有效地引导 LLM-as-a-Judge 输出更为详细的 CoT 推理。广泛的实验表明，我们的方法显著提高了评测的可靠性，在五个评测基准上平均提升了 6.7% 的准确率。此外，我们的方法还能生成更高质量的 CoT 推理，有助于进一步的 Judge 蒸馏，并无缝衔接地提升监督微调（SFT）的拒绝采样（rejection sampling）任务，我们称这种方法为 crowd rejection sampling，从而实现了更加高效的监督微调。进一步，我们分析验证了我们方法生成的 CoT 更加全面且质量更高，并且评测准确率会随测试时计算规模（test-time computation）的扩大而持续提高。 

论文：Structured Discourse Representation for Factual Consistency Verification

论文介绍：本文探索一种用于比较文本之间事实一致性的结构化的信息表示格式。该格式由原子事件和事件之间的discourse relation组成。每个原子事件由主语、谓语、宾语、状语、补语组成；每个discourse relation表示两个原子事件之间的时序，对比，或递进关系。基于这种结构化信息表示格式，本文首先对文本内容进行信息抽取，然后用分类模型判别每个原子事件或discourse relation是否被另一个文本包含，从而比较文本之间的事实一致性。本文在data-to-text generation和text summarisation两个任务上验证了所提出方法的有效性。 

论文：WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications

论文介绍：大型语言模型（LLMs）在广泛的任务中取得了令人印象深刻的成果，但它们在复杂的、特定领域的数学推理能力，特别是在无线通信领域，仍然未得到充分探索。在本研究中，我们介绍了WirelessMathBench，这是一个专门设计的基准，旨在评估LLMs在无线通信工程数学建模挑战中的表现。我们的基准包含587个精心策划的问题，来源于40篇最先进的研究论文，涵盖了从基本的选择题到复杂的方程补全任务的多样化任务，包括部分和完整补全，所有问题都严格遵循物理和维度约束。 

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
