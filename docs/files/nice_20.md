# NICE20期 | RNN卷土重来：基于门控记忆槽的线性注意力机制

## 主题

RNN卷土重来：基于门控记忆槽的线性注意力机制

## 时间

2024.7.28 10:30-11:30 周日

## 内容

内容
1. 背景 
    - 基于标准注意力机制的大语言模型
    - 线性化方法
2. 门控记忆槽注意力机制
    - KV memory视角下的attention及其线性化
    - 数据依赖的门控机制
    - 并行化方法
    - 参数化
3. 实验
    - 基准评测结果
    - GSA的Recall能力和隐状态容量分析
    - 继续训练的优势
4. FLA
5. 总结与展望
6. QA

引言
当前的大语言模型（LLM）在使用标准注意力机制时，面临着训练复杂度呈二次增长以及推理阶段管理键值（KV）缓存内存密集型的挑战。线性注意力作为一种有前景的替代方案，通过固定容量的隐藏状态取代了无界限的KV存储，从而缓解了这一问题。
然而，现有的线性注意力实现往往在性能上不及类似Llama架构（如Transformer++）的效率。
本次talk介绍了一种基于门控槽注意力（Gated Slot Attention，简称GSA）的方法，该方法融合了标准注意力的原理与数据依赖的门控线性注意力，实现了序列建模的线性化。通过更优的内存管理，采用GSA训练的LLM相较于以往的线性注意力设计应当展现出更好的性能。
更重要的是，GSA能够在现代硬件上实现高效并行化，这使得大规模实验成为可能。通过从头训练13亿和27亿参数的模型，GSA在一系列基准测试上表现出了很强的竞争力。此外，GSA与其他线性注意力变体相比，与现有的基于标准注意力的LLM有更好的兼容性。

### 入群

欢迎加入NICE每周分享交流群，可与NICEer唠嗑，以及第一时间收到后续NICE分享报告的通知。加群通过小助手认证，群内无广告。

<div align=center>
<img src="../images/nice_41_qr.png" width = "200">
<p>备注【昵称-单位-方向-NICE入群】</p>
</div>
